---
title: "BDA - Project"
author: "Jan Nyberg, Carl-Victor Schauman"
date: 4/12/2022
output: 
  pdf_document: 
    toc: yes
    toc_depth: 1
urlcolor: blue
---
\newpage

```{r, include=FALSE}
#Provided code and imports
library("rstan")
library(matrixStats)
library(loo)
set.seed(23)
ham_data = read.csv("ham_data.csv", header=FALSE)
```

# Introduction

For this project, we are trying to predict Lewis Hamilton's average score in a season based on previous years scores. This is mostly just due to our curiosity if we are able to use this to somehow predict the score.\
We are modeling his scores from five years, and trying to build a model using it. We want to see what kind of distribution the answer will be and how well it is able to estimate the following year. There are many factors we don't take into consideration, but we hope to see relatively good results and predictions.

# Description of the data

The data we use is from Kaggle and can be found [here](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020). We took Lewis Hamilton out of the data and chose the years 2015-2019. We selected his scores from all the races from those years. One thing to note with the data is that every year doesn't have an equal amount of races. To account for this we chose to fill in the missing races with the median for the year. This makes the data a bit inaccurate, however, it shouldn't have too big an effect on the data.

```{r, echo=FALSE}
num_races = seq(1, 21, by=1)
l <- as.list(as.data.frame(ham_data))
print(l)
cum_sums <- list()
for (i in l) {
  cum_sums[length(cum_sums)+1] <- list(cumsum(i[1:21]))
}
#print(cum_sums[[1]])
plot(num_races, cum_sums[[1]], type="l", main="Hamilton cumulative points",
    xlab="Races", ylab="Points")

lines(cum_sums[[2]], col="red")
lines(cum_sums[[3]], col="blue")
lines(cum_sums[[4]], col="yellow")
lines(cum_sums[[5]], col="green")
legend(
  x=2, y=400, legend=c("2015", "2016", "2017", "2018", "2019"),
  col=c("black", "red", "blue", "yellow", "green"),
  lty=1, cex=0.8
)
```

# Description of models

# Priors used

# Rstan code

## Hierarchical stan model

Below is the code for the hierarchical model for Hamilton's points.

```{stan, output.var="hier_ham"}
data {
  int<lower=0> N;
  int<lower=0> J;
  vector[J] y[N];
  real<lower=0> mu_s;
  real<lower=0> sigma_prior;
}

parameters {
  real<lower=0> mu;
  real<lower=0> sigma;
  real<lower=0> tau;
  vector[J] mus;
}

model {
  mu ~ normal(0, mu_s);
  tau ~ inv_chi_square(sigma_prior);
  sigma ~ gamma(1,1);
  mus ~ normal(mu, tau);
  for (j in 1:J)
    y[,j] ~ normal(mus[j], sigma);
}

generated quantities {
  vector[J] log_lik[N];
  real ypred;
  real ypred_6;
  ypred = normal_rng(mus[5], sigma);
  ypred_6 = normal_rng(mu, sigma);
  for (j in 1:J){
    for (n in 1:N){
      log_lik[n,j] = normal_lpdf(y[n,j] | mus[j], sigma);
    }
  }
}
```

## Non-hierarchical stan model

```{stan, output.var="pool_ham"}
data {
  int<lower=0> N;
  int<lower=0> J;
  vector[N*J] y;
  real mean_mu;
  real<lower=0> mean_sigma;
  
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  // prior
  mu ~ normal(mean_mu, mean_sigma);
  sigma ~ inv_chi_square(mean_sigma);
  // likelihood
  y ~ normal(mu, sigma);
}
generated quantities {
  real ypred;
  
  // Distribution based on all seasons
  ypred = normal_rng(mu, sigma);
}

```

# Running of stan model

## Hierarchical model

Below is the hierarchical model run with the corresponding histogram with the data.

```{r}
hier_data = list(
  y = ham_data,
  N = nrow(ham_data),
  J = ncol(ham_data),
  U = 26,
  mu_s = 20,
  sigma_prior = 7
)

hier_fit = sampling(
  hier_ham,
  data = hier_data,
  chains = 4,
  iter = 2000,
  warmup = 1000,
  refresh = 0
)
```

```{r, echo=FALSE}
ham_extracted = extract(hier_fit)$ypred
ham_extracted26 = ham_extracted[ham_extracted < 26]

hist(ham_extracted26, breaks = seq(min(ham_extracted26), max(ham_extracted26), length.out = 30), 
     xlab="Score", ylab="Density",main="Predictive distribution of the mean hamilton the next season", 
     col="lightblue", xlim=c(0,28))
abline(v = mean(ham_extracted), col="red", lwd=3)
abline(v = mean(ham_extracted26), col="blue", lwd=3)
legend(x=0.075, y=0.06, 
       legend=c("Mean without removing data","Mean with removing data"),
       col=c("red", "blue"), lty=1)

```

```{r, echo=FALSE}
ham_extracted_density = density(ham_extracted26)
plot(
  ham_extracted_density, xlim=c(0,28),
  main="Density plot of the mean hamilton the next season"
)
abline(v = mean(ham_extracted), col="red", lwd=3, xlab="")
abline(v = mean(ham_extracted), col="red", lwd=3)
abline(v = mean(ham_extracted26), col="blue", lwd=3)
legend(
  x=0.075, y=0.06, 
  legend=c("Mean without removing data","Mean with removing data"),
  col=c("red", "blue"), lty=1
)
```

## Nonhierarcial model (Pooled model)

```{r}
pool_data = list(
  y = unlist(ham_data),
  N = nrow(ham_data),
  J = ncol(ham_data),
  mean_mu = 18,
  mean_sigma = 6
)

pool_fit = sampling(
  pool_ham,
  data = pool_data,
  chains = 4,
  iter = 2000,
  warmup = 1000,
  refresh = 0
)
```

```{r, echo=FALSE}
ham_pool_extracted = extract(pool_fit)$ypred
ham_pool_extracted26 = ham_pool_extracted[ham_pool_extracted <= 26]

hist(ham_pool_extracted26, breaks = seq(min(ham_pool_extracted26), max(ham_pool_extracted26), length.out = 30), 
     xlab="Score", ylab="Density",main="Predictive distribution of the mean hamilton the next season", 
     col="lightblue", xlim=c(0,28))

abline(v = mean(ham_extracted), col="red", lwd=3)
abline(v = mean(ham_extracted26), col="blue", lwd=3)
legend(x=0.075, y=0.06, 
       legend=c("Mean without removing data","Mean with removing data"),
       col=c("red", "blue"), lty=1)
```

```{r, echo=FALSE}
ham_pool_extracted_density = density(ham_pool_extracted26)
plot(
  ham_pool_extracted_density,
  xlim=c(0,28), main="Density plot of the mean hamilton the next season"
)
abline(v = mean(ham_pool_extracted), col="red", lwd=3)
abline(v = mean(ham_pool_extracted26), col="blue", lwd=3)
legend(
  x=0.075, y=0.06, 
  legend=c("Mean without removing data","Mean with removing data"),
  col=c("red", "blue"), lty=1
)
```

Since the values of these normal distributions, go beyond the max points, i.e. 26, we have limited them a bit. We still plot the mean of both the limited and unlimited data. As can be seen, there isn't a lot of difference, however, over several races, this difference can be quite large. Below is also the histogram as a density plot.

# Convergence diagnostics

```{r, message=FALSE, warning=FALSE, echo=FALSE}
#mean(monitor(hier_fit)[["Tail_ESS"]][1:113])
hier_monitor = data.frame(monitor(hier_fit), print=FALSE)
pool_monitor = data.frame(monitor(pool_fit), print=FALSE)

r_hier_mean = mean(summary(hier_fit)$summary[1:113,10])
r_pool_mean = mean(summary(pool_fit)$summary[1:3,10])

```

The $\widehat{R}$ for our fits are as follows:

-   Hierarchical model: `r round(r_hier_mean, 2)`
-   Pooled model: `r round(r_pool_mean, 2)`

Since these $\widehat{R}$ values are under 1.05, the chains have most likely mixed well.

Another convergence diagnostic we can look at is the ESS value we get out of the fits.

-   Bulk ESS of the hierarchical model: `r mean(hier_monitor[["Bulk_ESS"]][1:113])`
-   Tail ESS of the hierarchical model: `r mean(hier_monitor[["Tail_ESS"]][1:113])`
-   Bulk ESS of the pooled model: `r mean(pool_monitor[["Bulk_ESS"]][1:3])`
-   Tail ESS of the pooled model: `r mean(pool_monitor[["Tail_ESS"]][1:3])`

These ESS values measure the cruse effective sample sice for the bulk and tail quantities. A value over 100 is good and all of our values are over it.

```{r}
traceplot(
  hier_fit,
  nrow=4,
  inc_warmup=FALSE,
  pars=c("mu", "sigma", "tau", "ypred")
)
```

```{r}
traceplot(
  pool_fit,
  nrow=3,
  inc_warmup=FALSE,
  pars=c("mu", "sigma", "ypred")
)
```

# Posterior predictive checks

# Predictive performance assessment

To assess the performance of the models we simulate 1000 seasons with the help of our models and compare the outcomes with the real world data.

| Year | Driver   | Points |
|------|----------|--------|
| 2015 | Hamilton | 413    |
| 2016 | Hamilton | 408    |
| 2017 | Hamilton | 363    |
| 2018 | Hamilton | 380    |
| 2019 | Hamilton | 381    |

```{r}
pool_season_preditcions = c()

for(i in 1:1000) {
  dens = ham_pool_extracted_density
  N <- 21
  newx <- sample(x = dens$x, N,
       prob = dens$y, replace=TRUE)
    + rnorm(N, 0, dens$bw)
  pool_season_preditcions <- append(pool_season_preditcions, sum(newx))
}

hist(
  pool_season_preditcions,
  breaks = seq(min(pool_season_preditcions),
               max(pool_season_preditcions),
               length.out = 30
            ),
  col="lightblue",
  xlim=c(200,450)
)
abline(v = 413, col="red", lwd=3)
abline(v = 408, col="red", lwd=3)
abline(v = 363, col="red", lwd=3)
abline(v = 380, col="red", lwd=3)
abline(v = 381, col="red", lwd=3)
```

```{r}
hier_season_preditcions = c()

for(i in 1:1000) {
  dens = ham_extracted_density
  N <- 21
  newx <- sample(x = dens$x, N,
       prob = dens$y, replace=TRUE)
    + rnorm(N, 0, dens$bw)
  hier_season_preditcions <- append(hier_season_preditcions, sum(newx))
}

hist(
  hier_season_preditcions,
  breaks = seq(min(hier_season_preditcions),
               max(hier_season_preditcions),
               length.out = 30
            ),
  col="lightblue",
  xlim=c(200,450)
)
abline(v = 413, col="red", lwd=3)
abline(v = 408, col="red", lwd=3)
abline(v = 363, col="red", lwd=3)
abline(v = 380, col="red", lwd=3)
abline(v = 381, col="red", lwd=3)
```

# Sensitivity analysis

# Model comparison

```{r}
df <- data.frame(ham_pool_extracted, ham_extracted)
colnames(df) <- c("points_pool", "points_hier")
ggplot(
  data = df,
  mapping = aes(x=points_pool)
) + geom_histogram(
  aes(x=points_pool, y=..density..),
  binwidth = 1,
  colour="black",
  fill="blue",
  position = "identity",
  alpha = 0.4
) + geom_histogram(
  aes(x=points_hier, y=..density..),
  binwidth = 1,
  colour="black",
  fill="green",
  position = "identity",
  alpha = 0.4
) + xlim(
  0,
  26
)

hier_extracted = extract_log_lik(hier_fit, merge_chains = FALSE)
r_eff = relative_eff(exp(hier_extracted))
hier_loo = loo(hier_extracted, r_eff = r_eff)
hier_elpd = hier_loo$estimates["elpd_loo",1]
paste("Hierarchical model PSIS-LOO elpd value: ", round(hier_elpd,1))
plot(hier_loo)
```

# Discussion of issues and potential improvements

# Conclusion what was learned from the data analysis

# Self-reflection
